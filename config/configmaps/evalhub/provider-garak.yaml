apiVersion: v1
kind: ConfigMap
metadata:
  name: evalhub-provider-garak
  labels:
    trustyai.opendatahub.io/evalhub-provider-type: system
    trustyai.opendatahub.io/evalhub-provider-name: garak
data:
  garak.yaml: |
    id: garak
    name: Garak
    description: LLM vulnerability scanner and red-teaming framework
    type: builtin
    runtime:
      k8s:
        image: $(evalhub-provider-garak-image)
        entrypoint:
        - python
        - /opt/app-root/src/main.py
        cpu_request: 100m
        memory_request: 128Mi
        cpu_limit: 500m
        memory_limit: 1Gi
        env:
        - name: VAR_NAME
          value: VALUE
      local: null
    benchmarks:
    - id: toxicity
      name: Toxicity Detection
      description: Tests model's tendency to generate toxic content
      category: safety
      metrics:
      - toxicity_rate
      - severity_score
      num_few_shot: 0
      dataset_size: 500
      tags:
      - safety
      - toxicity
      - red_team
    - id: bias_detection
      name: Bias Detection
      description: Evaluates model for various forms of bias
      category: fairness
      metrics:
      - bias_score
      - demographic_parity
      num_few_shot: 0
      dataset_size: 1000
      tags:
      - fairness
      - bias
      - demographic
    - id: pii_leakage
      name: PII Leakage
      description: Tests for personally identifiable information leakage
      category: privacy
      metrics:
      - pii_leak_rate
      - sensitivity_score
      num_few_shot: 0
      dataset_size: 300
      tags:
      - privacy
      - pii
      - security
    - id: prompt_injection
      name: Prompt Injection
      description: Tests resilience against prompt injection attacks
      category: security
      metrics:
      - injection_success_rate
      - defense_effectiveness
      num_few_shot: 0
      dataset_size: 200
      tags:
      - security
      - injection
      - adversarial
